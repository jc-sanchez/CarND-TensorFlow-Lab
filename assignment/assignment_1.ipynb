{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Assignment 1 - TensorFlow Neural Network</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image/notmnist.png\">\n",
    "In this assignment, we'll use all the tools we learned from Lesson 1 to run a neural network in TensorFlow.  The neural network will be classifying the letters A-J from different images.  These images are one of the letters A-J in a different font, as show in the above image.  At the end of this assignment, you'll be making predictions with at least an 80% accuracy!  Let's jump right in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start this assignment, we first need to import all the necessary modules.  Run the code below.  It will print \"All modules imported.\" after it has imported all the modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All modules imported.\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import os\n",
    "import pickle\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "print('All modules imported.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notMNIST data is a large dataset to handle for most computers.  It contains 500 thousands images for just training.  We'll be using a subset of this data, 21,000 images for each class(A-J)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files downloaded.\n"
     ]
    }
   ],
   "source": [
    "def download(url, file):\n",
    "    \"\"\"\n",
    "    Download file from <url>\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(file):\n",
    "        print('Downlading ' + file + '...')\n",
    "        urlretrieve(url, file)\n",
    "        print('Download Finished')\n",
    "\n",
    "# ToDo: Add URL\n",
    "# Download the training and test dataset\n",
    "download('', 'notMNIST_train.zip')\n",
    "download('', 'notMNIST_test.zip')\n",
    "\n",
    "print('All files downloaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210001/210001 [00:35<00:00, 5946.34files/s]\n",
      "100%|██████████| 10001/10001 [00:01<00:00, 6405.48files/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All features and labels uncompressed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def uncompress_features_labels(file):\n",
    "    \"\"\"\n",
    "    Uncompress features and labels from zip file\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    labels = []\n",
    "\n",
    "    with ZipFile(file) as zipf:\n",
    "        filenames_pbar = tqdm(zipf.namelist(), unit='files')\n",
    "        for filename in filenames_pbar:\n",
    "            # Check if the file is a directory\n",
    "            if not filename.endswith('/'):\n",
    "                with zipf.open(filename) as image_file:\n",
    "                    image = Image.open(image_file)\n",
    "                    image.load()\n",
    "                    # Load image data as 1 dimensional array\n",
    "                    # We're using float32 to save on memory\n",
    "                    feature = np.array(image, dtype=np.float32).flatten()\n",
    "\n",
    "                # Get the the letter from the filename\n",
    "                label = os.path.split(filename)[1][0]\n",
    "\n",
    "                features.append(feature)\n",
    "                labels.append(label)\n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "# Get the features and labels from the zip files\n",
    "train_features, train_labels = uncompress_features_labels('notMNIST_train.zip')\n",
    "test_features, test_labels = uncompress_features_labels('notMNIST_test.zip')\n",
    "\n",
    "print('All features and labels uncompressed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "The first problem involves normalizing the features for our training and test data.  To normalize the data, you need to modify the <i>normalize()</i> function to apply zero mean and zero variance scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed!\n"
     ]
    }
   ],
   "source": [
    "# Normalize the features\n",
    "# Apply zero mean and zero variance scale to the image features\n",
    "def normalize(data):\n",
    "    #  ToDo: Problem 1 - Implement function to normalize data\n",
    "\n",
    "train_features = normalize(train_features)\n",
    "test_features = normalize(test_features)\n",
    "\n",
    "# Test Cases\n",
    "np.testing.assert_array_almost_equal(\n",
    "    normalize(np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])),\n",
    "    np.array([-0.4, -0.3, -0.2, -0.099, 0.0, 0.099, 0.199, 0.3, 0.4, 0.5]),\n",
    "    decimal=3)\n",
    "np.testing.assert_array_almost_equal(\n",
    "    normalize(np.array([-100, -30, -1000, -20, -20, -10, -10, -20, -10, -10])),\n",
    "    np.array([9.5, 2.5, 99.5, 1.5, 1.5, 0.5, 0.5, 1.5, 0.5, 0.5]))\n",
    "\n",
    "print('Tests Passed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels One-Hot Encoded\n"
     ]
    }
   ],
   "source": [
    "# Turn labels into numbers and apply One-Hot Encoding\n",
    "encoder = LabelBinarizer()\n",
    "encoder.fit(train_labels)\n",
    "train_labels = encoder.transform(train_labels)\n",
    "test_labels = encoder.transform(test_labels)\n",
    "\n",
    "print('Labels One-Hot Encoded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features and labels randomized and split.\n"
     ]
    }
   ],
   "source": [
    "# Get randomized datasets for training and validation\n",
    "train_features, valid_features, train_labels, valid_labels = train_test_split(\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    test_size=0.05,\n",
    "    random_state=832289)\n",
    "\n",
    "print('Training features and labels randomized and split.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data to pickle file...\n",
      "Data cached in pickle file.\n"
     ]
    }
   ],
   "source": [
    "# Save the data for easy access\n",
    "pickle_file = 'notMNIST.pickle'\n",
    "if not os.path.isfile(pickle_file):\n",
    "    print('Saving data to pickle file...')\n",
    "    try:\n",
    "        with open('notMNIST.pickle', 'wb') as pfile:\n",
    "            pickle.dump(\n",
    "                {\n",
    "                    'train_dataset': train_features,\n",
    "                    'train_labels': train_labels,\n",
    "                    'valid_dataset': valid_features,\n",
    "                    'valid_labels': valid_labels,\n",
    "                    'test_dataset': test_features,\n",
    "                    'test_labels': test_labels,\n",
    "                },\n",
    "                pfile, pickle.HIGHEST_PROTOCOL)\n",
    "    except Exception as e:\n",
    "        print('Unable to save data to', pickle_file, ':', e)\n",
    "        raise\n",
    "\n",
    "print('Data cached in pickle file.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "For the neural network to train on our data, we need two tensors for input.  One tensor for the features and the other tensor for labels.\n",
    "** EDIT CODE **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = tf.placeholder(tf.float32, shape=(None, num_features))\n",
    "labels = tf.placeholder(tf.float32, shape=(None, num_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "Let's do that same for weights and biases.  Set one tensor for weights and one tensor for biases.\n",
    "** EDIT CODE **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = tf.Variable(tf.truncated_normal([num_features, num_labels]))\n",
    "biases = tf.Variable(tf.zeros([num_labels]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4\n",
    "** CONTINUE: The code below isn't working. **\n",
    "** EDIT CODE **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Linear Regression Function WX + b\n",
    "logits = tf.matmul(features, weights) + biases\n",
    "\n",
    "prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Cross entropy\n",
    "cross_entropy = -tf.reduce_sum(labels * tf.log(prediction), reduction_indices=1)\n",
    "\n",
    "# Training loss\n",
    "loss = tf.reduce_mean(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** CONTINUE: Talk about why we limit the training set **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Limit the amount of training data to use in the neural network.\n",
    "# Using all the training data will take too long to process.\n",
    "train_subset = 10000\n",
    "if len(train_features) > train_subset:\n",
    "    train_features = train_features[:train_subset, :]\n",
    "    train_labels = train_labels[:train_subset, :]\n",
    "\n",
    "# Gradient Descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)    \n",
    "\n",
    "# Determine if the predictions are correct\n",
    "is_correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(labels, 1))\n",
    "# Calculate the accuracy of the predictions\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct_prediction, tf.float32))\n",
    "\n",
    "# Create an operation that initializes all variables\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5\n",
    "Tweak the learning rate and number of steps to get a good accuracy. ** CONTINUE **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:   0  Loss:   16.557613372802734  Train Acc: 0.11180000007152557  Vald Acc:  0.10819047689437866\n",
      "Step: 100  Loss:    2.340564727783203  Train Acc: 0.7185999751091003  Vald Acc:   0.7080000042915344\n",
      "Step: 200  Loss:   1.8992466926574707  Train Acc: 0.7479000091552734  Vald Acc:   0.7286666631698608\n",
      "Step: 300  Loss:   1.6496416330337524  Train Acc: 0.7591999769210815  Vald Acc:   0.7366666793823242\n",
      "Step: 400  Loss:   1.4782439470291138  Train Acc: 0.7694000005722046  Vald Acc:   0.7405714392662048\n",
      "Step: 500  Loss:   1.3507778644561768  Train Acc: 0.7771000266075134  Vald Acc:   0.7442857027053833\n",
      "Step: 600  Loss:   1.2511628866195679  Train Acc: 0.78329998254776  Vald Acc:   0.7463809251785278\n",
      "Step: 700  Loss:   1.1708112955093384  Train Acc: 0.7903000116348267  Vald Acc:    0.748285710811615\n",
      "Step: 800  Loss:   1.1044509410858154  Train Acc: 0.7946000099182129  Vald Acc:   0.7492380738258362\n",
      "Test Acc: 0.8242999911308289\n",
      "Nice Job!\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1\n",
    "num_steps = 10\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "\n",
    "    # The training cycle\n",
    "    for step in range(num_steps):\n",
    "        # Run optimizer and get loss\n",
    "        _, l = session.run(\n",
    "            [optimizer, loss],\n",
    "            feed_dict={\n",
    "                features: train_features,\n",
    "                labels: train_labels})\n",
    "\n",
    "        # Display every 100 epochs\n",
    "        if not step % 100:\n",
    "            training_accuracy = session.run(\n",
    "                    accuracy,\n",
    "                    feed_dict={\n",
    "                        features: train_features,\n",
    "                        labels: train_labels})\n",
    "            validation_accuracy = session.run(\n",
    "                    accuracy,\n",
    "                    feed_dict={\n",
    "                        features: valid_features,\n",
    "                        labels: valid_labels})\n",
    "            print('Step: {:>3}  Loss: {:>20}  Train Acc: {:>5}  Vald Acc: {:>20}'.format(\n",
    "                step,\n",
    "                l,\n",
    "                training_accuracy,\n",
    "                validation_accuracy))\n",
    "\n",
    "    # Check accuracy against Test data\n",
    "    test_accuracy = session.run(\n",
    "        accuracy,\n",
    "        feed_dict={\n",
    "            features: test_features,\n",
    "            labels: test_labels})\n",
    "    print('Test Acc: {}'.format(test_accuracy))\n",
    "\n",
    "assert test_accuracy >= 0.80\n",
    "\n",
    "print('Nice Job!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
