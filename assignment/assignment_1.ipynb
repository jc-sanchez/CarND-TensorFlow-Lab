{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All modules imported.\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import os\n",
    "import pickle\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "print('All modules imported.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the notMNIST training and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files downloaded.\n"
     ]
    }
   ],
   "source": [
    "def download(url, file):\n",
    "    \"\"\"\n",
    "    Download file from <url>\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(file):\n",
    "        print('Downlading ' + file + '...')\n",
    "        urlretrieve(url, file)\n",
    "        print('Download Finished')\n",
    "\n",
    "# ToDo: Add URL\n",
    "# Download the training and test dataset\n",
    "download('', 'notMNIST_train.zip')\n",
    "download('', 'notMNIST_test.zip')\n",
    "\n",
    "print('All files downloaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the features and labels from the zip files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210001/210001 [00:34<00:00, 6100.99files/s]\n",
      "100%|██████████| 10001/10001 [00:01<00:00, 6392.77files/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All features and labels uncompressed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def uncompress_features_labels(file):\n",
    "    \"\"\"\n",
    "    Uncompress features and labels from zip file\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    labels = []\n",
    "\n",
    "    with ZipFile(file) as zipf:\n",
    "        filenames_pbar = tqdm(zipf.namelist(), unit='files')\n",
    "        for filename in filenames_pbar:\n",
    "            # Check if the file is a directory\n",
    "            if not filename.endswith('/'):\n",
    "                with zipf.open(filename) as image_file:\n",
    "                    image = Image.open(image_file)\n",
    "                    image.load()\n",
    "                    # Load image data as 1 dimensional array\n",
    "                    feature = np.array(image).flatten()\n",
    "\n",
    "                # Get the the letter from the filename\n",
    "                label = os.path.split(filename)[1][0]\n",
    "\n",
    "                features.append(feature)\n",
    "                labels.append(label)\n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "# Get the features and labels from the zip files\n",
    "train_features, train_labels = uncompress_features_labels('notMNIST_train.zip')\n",
    "test_features, test_labels = uncompress_features_labels('notMNIST_test.zip')\n",
    "\n",
    "print('All features and labels uncompressed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed!\n"
     ]
    }
   ],
   "source": [
    "# Normalize the features\n",
    "# Apply zero mean and zero variance scale to the image features\n",
    "def normalize_data(data):\n",
    "    #  ToDo: Problem 1 - Implement function to normalize data\n",
    "\n",
    "train_features = normalize_data(train_features)\n",
    "test_features = normalize_data(test_features)\n",
    "\n",
    "# Test Cases\n",
    "np.testing.assert_array_almost_equal(\n",
    "    normalize_data(np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])),\n",
    "    np.array([-0.4, -0.3, -0.2, -0.099, 0.0, 0.099, 0.199, 0.3, 0.4, 0.5]),\n",
    "    decimal=3)\n",
    "np.testing.assert_array_almost_equal(\n",
    "    normalize_data(np.array([-100, -30, -1000, -20, -20, -10, -10, -20, -10, -10])),\n",
    "    np.array([9.5, 2.5, 99.5, 1.5, 1.5, 0.5, 0.5, 1.5, 0.5, 0.5]))\n",
    "\n",
    "print('Tests Passed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features and labels randomized and split.\n"
     ]
    }
   ],
   "source": [
    "# Get randomized datasets for training and validation\n",
    "train_features, valid_features, train_labels, valid_labels = train_test_split(\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    test_size=0.05,\n",
    "    random_state=832289)\n",
    "\n",
    "print('Training features and labels randomized and split.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data to pickle file...\n",
      "File saved\n"
     ]
    }
   ],
   "source": [
    "# Save the data for easy access\n",
    "print('Saving data to pickle file...')\n",
    "pickle_file = 'notMNIST.pickle'\n",
    "if not os.path.isfile(pickle_file):\n",
    "    try:\n",
    "        with open('notMNIST.pickle', 'wb') as pfile:\n",
    "            pickle.dump(\n",
    "                {\n",
    "                    'train_dataset': train_features,\n",
    "                    'train_labels': train_labels,\n",
    "                    'valid_dataset': valid_features,\n",
    "                    'valid_labels': valid_labels,\n",
    "                    'test_dataset': test_features,\n",
    "                    'test_labels': test_labels,\n",
    "                },\n",
    "                pfile, pickle.HIGHEST_PROTOCOL)\n",
    "    except Exception as e:\n",
    "        print('Unable to save data to', pickle_file, ':', e)\n",
    "        raise\n",
    "\n",
    "print('Data cached in pickle file.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Todo: Create a simple example with tensorflow\n",
    "batch_size = 100\n",
    "learning_rate = 0.01\n",
    "epochs = 25\n",
    "\n",
    "\n",
    "def logistic_classifier(input):\n",
    "    \"\"\"\n",
    "    Create a logistic classifier\n",
    "    \"\"\"\n",
    "    weight = tf.Variable(tf.zeros([784, 10]))\n",
    "    bias = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "    # Linear Regression Function WX + b\n",
    "    logits = tf.matmul(input, weight) + bias\n",
    "\n",
    "    # Softmax turns the scores into probabilities, creating our prediction model\n",
    "    return tf.nn.softmax(logits)\n",
    "\n",
    "# The input is the images of letters. The images are 28 by 28 pixels. The image has a total size of 784. 28*28=784\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "# The labels are A-J, which make a total of 10 classes\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "prediction = logistic_classifier(x)\n",
    "\n",
    "# Cross entropy\n",
    "cross_entropy = -tf.reduce_sum(y * tf.log(prediction), reduction_indices=1)\n",
    "\n",
    "# Training loss\n",
    "cost = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "# Gradient Descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# Create an operation that initializes all variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # The training cycle\n",
    "    for epoch in range(epochs):\n",
    "        avg_cost = 0.\n",
    "        batch_count = int(len(train_features) / batch_size)\n",
    "\n",
    "        for batch_i in range(int(ceil(len(train_features)/float(batch_size)))):\n",
    "            front_batch = batch_i * batch_size\n",
    "            feature_batch = train_features[front_batch: front_batch + batch_size]\n",
    "            label_batch = train_labels[front_batch: front_batch + batch_size]\n",
    "\n",
    "            # Run optimization, then run the cost operation\n",
    "            _, c = sess.run(\n",
    "                [optimizer, cost],\n",
    "                feed_dict={x: feature_batch, y: label_batch})\n",
    "\n",
    "            avg_cost += c / batch_count\n",
    "        print('Epoch: {:>2} Cost: {}'.format(epoch, avg_cost))\n",
    "\n",
    "    correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: test_features, y: test_labels}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
